# CLMAndWWM
This repository contains the data and models from the paper '“Is Whole Word Masking Always Better for Chinese BERT?”: Probing on Chinese Grammatical Error Correction', which is accepted in Findings of ACL 2022, short. The paper is [here](https://arxiv.org/pdf/2203.00286.pdf).
#
The data was obtained from CGED's benchmark for Chinese Grammatical Error Diagnosis and subsequently processed by us.
The checkpoit of RoBERTa is [here](https://zhoucong-my.sharepoint.com/:f:/g/personal/o_xoffice_top/EhU3RN6qJjRAhY23nc14jOkB-_hm7kIOOCBxmMGdEwBvew?e=pxdAgt), you can load the model using hugginface interfaces.
